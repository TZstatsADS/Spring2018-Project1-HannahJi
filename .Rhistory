class(Franklin_speech_df)
install.packages("rJava")
library(rJava)
install.packages("qdap")
library(qdap)
packages_use<-c(
#"rJava",
#"openNLPdata",
"tidyverse",
"tidytext",
"dplyr",
"graphics",
"wordcloud2",
"tm",
"tidyr",
"ggplot2",
"lda",
"qdap",
"syuzhet"
)#list all packages I need
packages_need<-setdiff(
packages_use,
intersect(installed.packages()[,1],packages_use)
)#pick out the packages I need but haven't installed yet
if(length(packages_need)>0)  {install.packages(packages_need)}#If not exist, install them
#library all packages I need
#library(rJava)
#library(openNLPdata)
library(tibble)
library(tidytext)
library(dplyr)
library(graphics)
library(wordcloud2)
library(tm)
library(tidyr)
library(ggplot2)
library(lda)
#library(qdap)
library(syuzhet)
install.packages("qdap")
install.packages("rJava")
install.packages("rJava")
Franklin_speech_files<-list.files(path = "/Users/apple/Documents/2018SpringCourse/applied data science/Spring2018-Project1-HannahJi/data/InauguralSpeeches",full.names = T,pattern = "inaugFranklinDRoosevelt*")
#for(i in 1:length(Franklin_speech_files)){
#write_file<-file(Franklin_speech_files[i],"a")
#cat("\n",file=write_file)
#close(write_file)
#}
Franklin_speech_lists <-lapply(Franklin_speech_files,function(i) readLines(i))
Franklin_speech_df <- tibble(doc_num=1:length(Franklin_speech_files),text=Franklin_speech_lists)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
#apply(,1,unnest_tokens,word,as.character(text))#one-token-per-document-per-row
Franklin_speech_count <- Franklin_speech_token %>%
anti_join(stop_words)%>%
count(word,doc_num)#count word in every article
Franklin_speech_count<-Franklin_speech_count[order(Franklin_speech_count$doc_num,Franklin_speech_count$n,decreasing=TRUE),]
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)]$n>=5,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==2,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==2,c(1,3)]$n>=5,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==3,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==3,c(1,3)]$n>=7,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==4,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==4,c(1,3)]$n>=5,'red','skyblue'))
bing_word_counts<-NULL
for(i in 1:4){
new <- Franklin_speech_token[Franklin_speech_token$doc_num==i,] %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment,sort = TRUE) #%>%
#ungroup()
new$doc_num<-i
bing_word_counts<-rbind(bing_word_counts,new)
}
tbl_df(bing_word_counts) %>%
group_by(doc_num,sentiment) %>%
top_n(3,n) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_grid(doc_num~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()+
theme(axis.text=element_text(size=3))
Franklin_speech_sentiment <- Franklin_speech_df %>%
unnest_tokens(word, text)%>%
group_by(doc_num)%>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
#Detail in each speech
ggplot(Franklin_speech_sentiment) +
geom_col(aes(word, sentiment,fill=doc_num),show.legend = FALSE)+
facet_wrap(~doc_num, ncol = 2, scales = "free_x")+
theme(axis.text.x = element_blank())+
theme(axis.ticks = element_blank())+
labs(title = "Sentiment of four FranklinDRoosevelt's inaugurations")
sentence.list=NULL
for(i in 1:nrow(Franklin_speech_df)){
sentences=sent_detect(Franklin_speech_df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=data.frame(rbind(sentence.list,
cbind(sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences),
doc_num=rep(i,length(sentences))
)))
}
}
Franklin_speech_files<-list.files(path = "/Users/apple/Documents/2018SpringCourse/applied data science/Spring2018-Project1-HannahJi/data/InauguralSpeeches",full.names = T,pattern = "inaugFranklinDRoosevelt*")
#for(i in 1:length(Franklin_speech_files)){
#write_file<-file(Franklin_speech_files[i],"a")
#cat("\n",file=write_file)
#close(write_file)
#}
Franklin_speech_lists <-lapply(Franklin_speech_files,function(i) readLines(i))
Franklin_speech_df <- tibble(doc_num=1:length(Franklin_speech_files),text=Franklin_speech_lists)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
#apply(,1,unnest_tokens,word,as.character(text))#one-token-per-document-per-row
Franklin_speech_count <- Franklin_speech_token %>%
anti_join(stop_words)%>%
count(word,doc_num)#count word in every article
Franklin_speech_count<-Franklin_speech_count[order(Franklin_speech_count$doc_num,Franklin_speech_count$n,decreasing=TRUE),]
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)]$n>=5,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==2,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==2,c(1,3)]$n>=5,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==3,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==3,c(1,3)]$n>=7,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==4,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==4,c(1,3)]$n>=5,'red','skyblue'))
bing_word_counts<-NULL
for(i in 1:4){
new <- Franklin_speech_token[Franklin_speech_token$doc_num==i,] %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment,sort = TRUE) #%>%
#ungroup()
new$doc_num<-i
bing_word_counts<-rbind(bing_word_counts,new)
}
tbl_df(bing_word_counts) %>%
group_by(doc_num,sentiment) %>%
top_n(3,n) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_grid(doc_num~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()+
theme(axis.text=element_text(size=3))
Franklin_speech_sentiment <- Franklin_speech_df %>%
unnest_tokens(word, text)%>%
group_by(doc_num)%>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
#Detail in each speech
ggplot(Franklin_speech_sentiment) +
geom_col(aes(word, sentiment,fill=doc_num),show.legend = FALSE)+
facet_wrap(~doc_num, ncol = 2, scales = "free_x")+
theme(axis.text.x = element_blank())+
theme(axis.ticks = element_blank())+
labs(title = "Sentiment of four FranklinDRoosevelt's inaugurations")
sentence.list=NULL
for(i in 1:nrow(Franklin_speech_df)){
sentences=sent_detect(Franklin_speech_df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=data.frame(rbind(sentence.list,
cbind(sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences),
doc_num=rep(i,length(sentences))
)))
}
}
doc_topicmodel<-NULL
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list))]
corpus.list$snippets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")#each snippet is the present sentence combined with its previous and post ones
docs <- Corpus(VectorSource(corpus.list$snippets))
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
#remove punctuation
docs <- tm_map(docs, removePunctuation)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#Stem document
docs <- tm_map(docs,stemDocument)
#doc_topicmodel<-rbind(doc_topicmodel,cbind(data.frame(docs),doc_num=rep(i,nrow(data.frame(docs)))))
dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 3
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best,burnin = burnin, iter = iter, thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
setwd("/Users/apple/Documents/2018SpringCourse/Applied Data Science/Spring2018-Project1-HannahJi/output")
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv",sep=""))
ldaOut.terms<- as.matrix(terms(ldaOut,10))
write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicProbabilities-.csv",sep=""),append=TRUE)
print(ldaOut.terms)
Franklin_speech_df
data.frame(Franklin_speech_df)
View(data.frame(Franklin_speech_df))
Franklin_speech_files<-list.files(path = "/Users/apple/Documents/2018SpringCourse/applied data science/Spring2018-Project1-HannahJi/data/InauguralSpeeches",full.names = T,pattern = "inaugFranklinDRoosevelt*")
#for(i in 1:length(Franklin_speech_files)){
#write_file<-file(Franklin_speech_files[i],"a")
#cat("\n",file=write_file)
#close(write_file)
#}
Franklin_speech_lists <-lapply(Franklin_speech_files,function(i) readLines(i))
Franklin_speech_df <- tibble(doc_num=1:length(Franklin_speech_files),text=Franklin_speech_lists)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
#apply(,1,unnest_tokens,word,as.character(text))#one-token-per-document-per-row
Franklin_speech_count <- Franklin_speech_token %>%
anti_join(stop_words)%>%
count(word,doc_num)#count word in every article
Franklin_speech_count<-Franklin_speech_count[order(Franklin_speech_count$doc_num,Franklin_speech_count$n,decreasing=TRUE),]
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)]$n>=5,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==2,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==2,c(1,3)]$n>=5,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==3,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==3,c(1,3)]$n>=7,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==4,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==4,c(1,3)]$n>=5,'red','skyblue'))
bing_word_counts<-NULL
for(i in 1:4){
new <- Franklin_speech_token[Franklin_speech_token$doc_num==i,] %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment,sort = TRUE) #%>%
#ungroup()
new$doc_num<-i
bing_word_counts<-rbind(bing_word_counts,new)
}
tbl_df(bing_word_counts) %>%
group_by(doc_num,sentiment) %>%
top_n(3,n) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_grid(doc_num~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()+
theme(axis.text=element_text(size=3))
Franklin_speech_sentiment <- Franklin_speech_df %>%
unnest_tokens(word, text)%>%
group_by(doc_num)%>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
#Detail in each speech
ggplot(Franklin_speech_sentiment) +
geom_col(aes(word, sentiment,fill=doc_num),show.legend = FALSE)+
facet_wrap(~doc_num, ncol = 2, scales = "free_x")+
theme(axis.text.x = element_blank())+
theme(axis.ticks = element_blank())+
labs(title = "Sentiment of four FranklinDRoosevelt's inaugurations")
sentence.list=NULL
for(i in 1:nrow(Franklin_speech_df)){
sentences=sent_detect(Franklin_speech_df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=data.frame(rbind(sentence.list,
cbind(sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences),
doc_num=rep(i,length(sentences))
)))
}
}
doc_topicmodel<-NULL
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list))]
corpus.list$snippets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")#each snippet is the present sentence combined with its previous and post ones
docs <- Corpus(VectorSource(corpus.list$snippets))
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
#remove punctuation
docs <- tm_map(docs, removePunctuation)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#Stem document
docs <- tm_map(docs,stemDocument)
#doc_topicmodel<-rbind(doc_topicmodel,cbind(data.frame(docs),doc_num=rep(i,nrow(data.frame(docs)))))
dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 3
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best,burnin = burnin, iter = iter, thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
setwd("/Users/apple/Documents/2018SpringCourse/Applied Data Science/Spring2018-Project1-HannahJi/output")
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv",sep=""))
ldaOut.terms<- as.matrix(terms(ldaOut,10))
write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicProbabilities-.csv",sep=""),append=TRUE)
print(ldaOut.terms)
install.packages("tibble",repos="https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html")
install.packages("tibble", repos = "https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html")
install.packages("tibble",repos="https://CRAN.R-project.org/package=tibble")
install.packages("tibble", repos = "https://CRAN.R-project.org/package=tibble")
install.packages("tibble",repos="https://CRAN.R-project.org/package=tibble",dependencies = TRUE,type="source")
install.packages("tibble", repos = "https://CRAN.R-project.org/package=tibble", dependencies = TRUE, type = "source")
install.packages("tibble",dependencies = TRUE,type="source")
library(tibble)
Franklin_speech_files<-list.files(path = "/Users/apple/Documents/2018SpringCourse/applied data science/Spring2018-Project1-HannahJi/data/InauguralSpeeches",full.names = T,pattern = "inaugFranklinDRoosevelt*")
#for(i in 1:length(Franklin_speech_files)){
#write_file<-file(Franklin_speech_files[i],"a")
#cat("\n",file=write_file)
#close(write_file)
#}
Franklin_speech_lists <-lapply(Franklin_speech_files,function(i) readLines(i))
Franklin_speech_df <- tibble(doc_num=1:length(Franklin_speech_files),text=Franklin_speech_lists)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
Franklin_speech_files<-list.files(path = "/Users/apple/Documents/2018SpringCourse/applied data science/Spring2018-Project1-HannahJi/data/InauguralSpeeches",full.names = T,pattern = "inaugFranklinDRoosevelt*")
#for(i in 1:length(Franklin_speech_files)){
#write_file<-file(Franklin_speech_files[i],"a")
#cat("\n",file=write_file)
#close(write_file)
#}
Franklin_speech_lists <-lapply(Franklin_speech_files,function(i) readLines(i))
Franklin_speech_df <- tibble(doc_num=1:length(Franklin_speech_files),text=Franklin_speech_lists)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
install.packages("rJava")
library(rJava)
Franklin_speech_files<-list.files(path = "/Users/apple/Documents/2018SpringCourse/applied data science/Spring2018-Project1-HannahJi/data/InauguralSpeeches",full.names = T,pattern = "inaugFranklinDRoosevelt*")
#for(i in 1:length(Franklin_speech_files)){
#write_file<-file(Franklin_speech_files[i],"a")
#cat("\n",file=write_file)
#close(write_file)
#}
Franklin_speech_lists <-lapply(Franklin_speech_files,function(i) readLines(i))
Franklin_speech_df <- tibble(doc_num=1:length(Franklin_speech_files),text=Franklin_speech_lists)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
library(dplyr)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
?unnest_tokens
#library all packages I need
#library(rJava)
library(tidytext)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
Franklin_speech_files<-list.files(path = "/Users/apple/Documents/2018SpringCourse/applied data science/Spring2018-Project1-HannahJi/data/InauguralSpeeches",full.names = T,pattern = "inaugFranklinDRoosevelt*")
#for(i in 1:length(Franklin_speech_files)){
#write_file<-file(Franklin_speech_files[i],"a")
#cat("\n",file=write_file)
#close(write_file)
#}
Franklin_speech_lists <-lapply(Franklin_speech_files,function(i) readLines(i))
Franklin_speech_df <- tibble(doc_num=1:length(Franklin_speech_files),text=Franklin_speech_lists)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
#apply(,1,unnest_tokens,word,as.character(text))#one-token-per-document-per-row
Franklin_speech_count <- Franklin_speech_token %>%
anti_join(stop_words)%>%
count(word,doc_num)#count word in every article
Franklin_speech_count<-Franklin_speech_count[order(Franklin_speech_count$doc_num,Franklin_speech_count$n,decreasing=TRUE),]
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)]$n>=5,'red','skyblue'))
#library all packages I need
#library(rJava)
library(tidytext)
library(dplyr)
library(graphics)
library(wordcloud2)
library(wordcloud2)
library(tm)
library(tidyr)
library(ggplot2)
library(lda)
#library(qdap)
library(syuzhet)
Franklin_speech_files<-list.files(path = "/Users/apple/Documents/2018SpringCourse/applied data science/Spring2018-Project1-HannahJi/data/InauguralSpeeches",full.names = T,pattern = "inaugFranklinDRoosevelt*")
#for(i in 1:length(Franklin_speech_files)){
#write_file<-file(Franklin_speech_files[i],"a")
#cat("\n",file=write_file)
#close(write_file)
#}
Franklin_speech_lists <-lapply(Franklin_speech_files,function(i) readLines(i))
Franklin_speech_df <- tibble(doc_num=1:length(Franklin_speech_files),text=Franklin_speech_lists)
Franklin_speech_token<-Franklin_speech_df%>%
unnest_tokens(word,text)
#apply(,1,unnest_tokens,word,as.character(text))#one-token-per-document-per-row
Franklin_speech_count <- Franklin_speech_token %>%
anti_join(stop_words)%>%
count(word,doc_num)#count word in every article
Franklin_speech_count<-Franklin_speech_count[order(Franklin_speech_count$doc_num,Franklin_speech_count$n,decreasing=TRUE),]
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==1,c(1,3)]$n>=5,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==2,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==2,c(1,3)]$n>=5,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==3,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==3,c(1,3)]$n>=7,'red','skyblue'))
wordcloud2(Franklin_speech_count[Franklin_speech_count$doc_num==4,c(1,3)],size=0.5,color=ifelse(Franklin_speech_count[Franklin_speech_count$doc_num==4,c(1,3)]$n>=5,'red','skyblue'))
bing_word_counts<-NULL
for(i in 1:4){
new <- Franklin_speech_token[Franklin_speech_token$doc_num==i,] %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment,sort = TRUE) #%>%
#ungroup()
new$doc_num<-i
bing_word_counts<-rbind(bing_word_counts,new)
}
tbl_df(bing_word_counts) %>%
group_by(doc_num,sentiment) %>%
top_n(3,n) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_grid(doc_num~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()+
theme(axis.text=element_text(size=3))
Franklin_speech_sentiment <- Franklin_speech_df %>%
unnest_tokens(word, text)%>%
group_by(doc_num)%>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
#Detail in each speech
ggplot(Franklin_speech_sentiment) +
geom_col(aes(word, sentiment,fill=doc_num),show.legend = FALSE)+
facet_wrap(~doc_num, ncol = 2, scales = "free_x")+
theme(axis.text.x = element_blank())+
theme(axis.ticks = element_blank())+
labs(title = "Sentiment of four FranklinDRoosevelt's inaugurations")
sentence.list=NULL
for(i in 1:nrow(Franklin_speech_df)){
sentences=sent_detect(Franklin_speech_df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=data.frame(rbind(sentence.list,
cbind(sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences),
doc_num=rep(i,length(sentences))
)))
}
}
library(qdap)
library(rJava)
install.packages("rJava")
library(rJava)
remove.packages(rJava)
remove.packages("rJava")
remove.packages("Rweka")
install.packages("rJava", dependencies = TRUE, type = "source")
library(rJava)
install.packages("rJava")
library(rJava)
